{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eeef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from collections import defaultdict, Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef09293",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.split = 'non-overlap'\n",
    "    config.slide_step = 12\n",
    "    config.hidden_size = 64 # 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 256 # 3072\n",
    "    config.transformer.num_heads = 8\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "\n",
    "config = get_b16_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
    "                                         width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.hybrid:\n",
    "            x = self.hybrid_model(x)\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "        return encoded, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=True):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.head = Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x, attn_weights = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits, attn_weights\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.weight)\n",
    "                nn.init.zeros_(self.head.bias)\n",
    "            else:\n",
    "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
    "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
    "\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                if bname.startswith('part') == False:\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af166af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then cosine decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d13a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        # if not os.path.exists(self.dataset_dir):\n",
    "            # raise ValueError(f'{self.dataset_dir} does not exist yet. Please generate the dataset first.')\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "trainset = celebADataset(split = \"train\", transform = transform_train)\n",
    "valset = celebADataset(split = \"val\", transform = transform_train)\n",
    "testset = celebADataset(split = \"test\", transform = transform_test)\n",
    "\n",
    "train_sampler = RandomSampler(trainset) \n",
    "val_sampler = SequentialSampler(valset)\n",
    "test_sampler = SequentialSampler(testset)\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          sampler=train_sampler,\n",
    "                          batch_size=train_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "val_loader = DataLoader(valset,\n",
    "                          sampler=val_sampler,\n",
    "                          batch_size=eval_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                         sampler=test_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30df4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionTransformer(config, img_size = 224, zero_head=True, num_classes=2)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-2, momentum=0.9, weight_decay=0)\n",
    "\n",
    "model.to(device)\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = 20000\n",
    "warmup_steps = 100\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921383c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1\n",
    "eval_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    checkpoint_path = \"models/greyhair_scratch.bin\"\n",
    "    torch.save(model_to_save.state_dict(), checkpoint_path)\n",
    "    print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e447345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff99e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        loss = model(x, y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scheduler.step()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val)\n",
    "            )\n",
    "            \n",
    "        if global_step % eval_every == 0:\n",
    "            val_acc = valid(model, val_loader)\n",
    "            if best_acc < val_acc:\n",
    "                save_model(model)\n",
    "                best_acc = val_acc\n",
    "                print(\"current best val accuracy: %2.5f\" % best_acc)\n",
    "            model.train()\n",
    "            \n",
    "        if global_step % t_total == 0:\n",
    "            break\n",
    "            \n",
    "    losses.reset()\n",
    "    if global_step % t_total == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee004afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"models/greyhair_scratch.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda()\n",
    "accuracy = valid(model, train_loader)\n",
    "print(\"Train accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "model.eval().cuda()\n",
    "accuracy = valid(model, val_loader)\n",
    "print(\"Val accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda()\n",
    "accuracy = valid(model, test_loader)\n",
    "print(\"Test accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd865958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, sensitive, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.sensitive = sensitive\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        # if not os.path.exists(self.dataset_dir):\n",
    "            # raise ValueError(f'{self.dataset_dir} does not exist yet. Please generate the dataset first.')\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['Male']==self.sensitive]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a175e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "testset_0 = celebADataset(split = \"test\", sensitive = 0, transform = transform_test)\n",
    "\n",
    "test_0_sampler = SequentialSampler(testset_0)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_0_loader = DataLoader(testset_0,\n",
    "                         sampler=test_0_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)\n",
    "\n",
    "testset_1 = celebADataset(split = \"test\", sensitive = 1, transform = transform_test)\n",
    "\n",
    "test_1_sampler = SequentialSampler(testset_1)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_1_loader = DataLoader(testset_1,\n",
    "                         sampler=test_1_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    \n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_preds, all_label).ravel()\n",
    "\n",
    "    return accuracy, tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, test_loader):\n",
    "    model.eval().cuda()\n",
    "    acc, tn, fp, fn, tp = valid(model, test_loader)\n",
    "    print(\"Accuracy: %2.5f\" % acc)\n",
    "    \n",
    "    print(\"Confusion matrix: \\n\",\n",
    "    \"True Positive: %s \\t False Positive: %s \\n\" %(tp, fp),\n",
    "    \"False Negative: %s \\t True Negative: %s\" %(fn, tn))\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    print(\"True Positive Rate: %2.5f\" % tpr)\n",
    "    print(\"True Negative Rate: %2.5f\" % tnr)\n",
    "\n",
    "    fpr = fp / (tn + fp)\n",
    "    fnr = fn / (tp + fn)\n",
    "    \n",
    "    print(\"False Positive Rate: %2.5f\" % fpr)\n",
    "    print(\"False Negative Rate: %2.5f\" % fnr)\n",
    "    \n",
    "    return acc, tpr, tnr, fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc0, tpr0, tnr0, fpr0, fnr0 = get_results(model, test_0_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e869d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1, tpr1, tnr1, fpr1, fnr1 = get_results(model, test_1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5065875",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.5 * (acc0 + acc1 )\n",
    "print(\"Accuracy: %2.5f\" % acc)\n",
    "\n",
    "dp_acc = abs(acc0 - acc1)\n",
    "print(\"DP_Acc: %2.5f\" % dp_acc)\n",
    "\n",
    "dp = abs(tpr1 - tpr0)\n",
    "print(\"DP: %2.5f\" % dp)\n",
    "\n",
    "ba = 0.25*(tpr0 + tnr0 + tpr1 + tnr1)\n",
    "print(\"BA: %2.5f\" % ba)\n",
    "\n",
    "eqodds = 0.5 * abs(tpr1 - tpr0) + 0.5 * abs(fpr1 - fpr0)\n",
    "print(\"Equal Odds: %2.5f\" % eqodds)\n",
    "\n",
    "diff_ba = 0.5*(tpr0 + tnr0) - 0.5*(tpr1 + tnr1)\n",
    "print(\"Difference BA: %2.5f\" % abs(diff_ba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
