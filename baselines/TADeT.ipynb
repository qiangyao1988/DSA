{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as pjoin\n",
    "from collections import defaultdict, Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Function\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef09293",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.split = 'non-overlap'\n",
    "    config.slide_step = 12\n",
    "    config.hidden_size = 64 # 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 256 # 3072\n",
    "    config.transformer.num_heads = 8\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "\n",
    "config = get_b16_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return mixed_query_layer, attention_output, weights\n",
    "\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
    "                                         width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.hybrid:\n",
    "            x = self.hybrid_model(x)\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        q, x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return q, x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            q, hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return q, encoded, attn_weights\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        q, encoded, attn_weights = self.encoder(embedding_output)\n",
    "        return q, encoded, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=True):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.head = Linear(config.hidden_size, num_classes)\n",
    "        self.sensitive_head = Linear(config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, alpha, labels=None):\n",
    "        q, x, attn_weights = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "        \n",
    "        reverse_feature = ReverseLayerF.apply(q, alpha)\n",
    "        \n",
    "        sensitive_logits = self.sensitive_head(reverse_feature[:, 0])\n",
    "      \n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits, sensitive_logits, q[:,0], attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af166af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then cosine decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customsampler(RandomSampler):\n",
    "\n",
    "    def __init__(self, data_source, replacement=False, num_samples=None, batch_size=None, generator=None):\n",
    "        super(Customsampler, self).__init__(data_source=data_source, replacement=replacement,\n",
    "                                            num_samples=num_samples, generator=generator)\n",
    "        \n",
    "        self.l = 2\n",
    "        self.g = 2\n",
    "        self.nbatch_size = batch_size // (self.l*self.g)\n",
    "        self.num_data = data_source.num_data\n",
    "        pos = np.unravel_index(np.argmax(self.num_data), self.num_data.shape)\n",
    "        self.max_pos = pos[0] * self.g + pos[1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        final_list = []\n",
    "        index_list = []\n",
    "        total_num = 0\n",
    "        for i in range(self.l*self.g):\n",
    "            tmp = np.arange(self.num_data[i//self.l, i%self.l]) + total_num\n",
    "            np.random.shuffle(tmp)\n",
    "            index_list.append(list(tmp))\n",
    "            if i != self.max_pos:\n",
    "                while len(index_list[-1]) < np.max(self.num_data):\n",
    "                    tmp = np.arange(self.num_data[i//self.l, i%self.l]) + total_num\n",
    "                    np.random.shuffle(tmp)                    \n",
    "                    index_list[-1].extend(list(tmp))\n",
    "            total_num += self.num_data[i//self.l, i%self.l]\n",
    "\n",
    "        for tmp in range(len(index_list[self.max_pos]) // self.nbatch_size):\n",
    "            for list_ in index_list:\n",
    "                final_list.extend(list_[tmp*self.nbatch_size:(tmp+1)*self.nbatch_size])\n",
    "\n",
    "        return iter(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d13a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        # if not os.path.exists(self.dataset_dir):\n",
    "            # raise ValueError(f'{self.dataset_dir} does not exist yet. Please generate the dataset first.')\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.num_data = self._data_count()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y, s\n",
    "    \n",
    "    def _data_count(self):\n",
    "        data_count = np.zeros((2, 2), dtype=int)\n",
    "        # print('<data_count> %s mode'%self.split)\n",
    "        for index in range(len(self.metadata_df)):\n",
    "            target = self.metadata_df.iloc[index]['Gray_Hair']\n",
    "            sensitive = self.metadata_df.iloc[index]['Male']\n",
    "            data_count[sensitive, target] += 1\n",
    "        # for i in range(2):\n",
    "            # print('# of %d groups data : '%i, data_count[i, :])\n",
    "        return data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "trainset = celebADataset(split = \"train\", transform = transform_train)\n",
    "valset = celebADataset(split = \"val\", transform = transform_train)\n",
    "testset = celebADataset(split = \"test\", transform = transform_test)\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "# train_sampler = RandomSampler(trainset) \n",
    "train_sampler = Customsampler(trainset, replacement=False, batch_size=train_batch_size)\n",
    "val_sampler = SequentialSampler(valset)\n",
    "test_sampler = SequentialSampler(testset)\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          sampler=train_sampler,\n",
    "                          batch_size=train_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "val_loader = DataLoader(valset,\n",
    "                          sampler=val_sampler,\n",
    "                          batch_size=eval_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                         sampler=test_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30df4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionTransformer(config, img_size = 224, zero_head=True, num_classes=2)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-2, momentum=0.9, weight_decay=0)\n",
    "\n",
    "model.to(device)\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = 20000\n",
    "warmup_steps = 100\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921383c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = AverageMeter()\n",
    "global_step, best_acc = 0, 0\n",
    "gradient_accumulation_steps = 1\n",
    "max_grad_norm = 1\n",
    "eval_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(feature1, feature2):\n",
    "    return (feature1 - feature2).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    checkpoint_path = \"models/greyhair_tadet.bin\"\n",
    "    torch.save(model_to_save.state_dict(), checkpoint_path)\n",
    "    print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e447345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x, alpha = None)[0]\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff99e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_loader,\n",
    "                          desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        p = float(step * len(epoch_iterator)) / t_total / len(epoch_iterator)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        cls_logits, sensitive_logits, q, _ = model(x, alpha)\n",
    "        \n",
    "        cls_loss = loss_fct(cls_logits, y)\n",
    "        adv_loss = loss_fct(sensitive_logits, s)\n",
    "        mse_loss = mse(q[s == 0].mean(), q[s == 1].mean())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = cls_loss + adv_loss + mse_loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            losses.update(loss.item() * gradient_accumulation_steps)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scheduler.step()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_iterator.set_description(\n",
    "                \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val)\n",
    "            )\n",
    "            \n",
    "        if global_step % eval_every == 0:\n",
    "            val_acc = valid(model, val_loader)\n",
    "            if best_acc < val_acc:\n",
    "                save_model(model)\n",
    "                best_acc = val_acc\n",
    "                print(\"current best val accuracy: %2.5f\" % best_acc)\n",
    "            model.train()\n",
    "            \n",
    "        if global_step % t_total == 0:\n",
    "            break\n",
    "            \n",
    "    losses.reset()\n",
    "    if global_step % t_total == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee004afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"models/greyhair_tadet.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval().cuda()\n",
    "# accuracy = valid(model, train_loader)\n",
    "# print(\"Train accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "# model.eval().cuda()\n",
    "# accuracy = valid(model, val_loader)\n",
    "# print(\"Val accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval().cuda()\n",
    "# accuracy = valid(model, test_loader)\n",
    "# print(\"Test accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, sensitive, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.sensitive = sensitive\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        # if not os.path.exists(self.dataset_dir):\n",
    "            # raise ValueError(f'{self.dataset_dir} does not exist yet. Please generate the dataset first.')\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['Male']==self.sensitive]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "testset_0 = celebADataset(split = \"test\", sensitive = 0, transform = transform_test)\n",
    "\n",
    "test_0_sampler = SequentialSampler(testset_0)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_0_loader = DataLoader(testset_0,\n",
    "                         sampler=test_0_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)\n",
    "\n",
    "testset_1 = celebADataset(split = \"test\", sensitive = 1, transform = transform_test)\n",
    "\n",
    "test_1_sampler = SequentialSampler(testset_1)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_1_loader = DataLoader(testset_1,\n",
    "                         sampler=test_1_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x, alpha = None)[0]\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    \n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_preds, all_label).ravel()\n",
    "\n",
    "    return accuracy, tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, test_loader):\n",
    "    model.eval().cuda()\n",
    "    acc, tn, fp, fn, tp = valid(model, test_loader)\n",
    "    print(\"Accuracy: %2.5f\" % acc)\n",
    "    \n",
    "    print(\"Confusion matrix: \\n\",\n",
    "    \"True Positive: %s \\t False Positive: %s \\n\" %(tp, fp),\n",
    "    \"False Negative: %s \\t True Negative: %s\" %(fn, tn))\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    print(\"True Positive Rate: %2.5f\" % tpr)\n",
    "    print(\"True Negative Rate: %2.5f\" % tnr)\n",
    "\n",
    "    fpr = fp / (tn + fp)\n",
    "    fnr = fn / (tp + fn)\n",
    "    \n",
    "    print(\"False Positive Rate: %2.5f\" % fpr)\n",
    "    print(\"False Negative Rate: %2.5f\" % fnr)\n",
    "    \n",
    "    return acc, tpr, tnr, fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc0, tpr0, tnr0, fpr0, fnr0 = get_results(model, test_0_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ec362",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1, tpr1, tnr1, fpr1, fnr1 = get_results(model, test_1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a854ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.5 * (acc0 + acc1 )\n",
    "print(\"Accuracy: %2.5f\" % acc)\n",
    "\n",
    "ba = 0.25*(tpr0 + tnr0 + tpr1 + tnr1)\n",
    "print(\"BA: %2.5f\" % ba)\n",
    "\n",
    "eqodds = 0.5 * (tpr1 - tpr0) + 0.5 * ((fpr1 - fpr0))\n",
    "print(\"Equal Odds: %2.5f\" % eqodds)\n",
    "\n",
    "diff_ba = 0.5*(tpr0 + tnr0) - 0.5*(tpr1 + tnr1)\n",
    "print(\"Difference BA: %2.5f\" % abs(diff_ba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79356836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
